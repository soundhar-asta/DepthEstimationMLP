/Users/soundharkk/Downloads/aidi_1002_final_project_template_(3).py# -*- coding: utf-8 -*-
"""AIDI_1002_Final_Project_Template (3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ULo24Nb3lxR9zLHZgWWB4eNLvQjwWqQC

# Title: AIDI 1002 Final Term Project Report

#### Members' Names:
    Soundhar Keerambur Kanagarajan (200573263)
    Surya Prakash Vadivel (200573940)

####  Emails:
    Soundhar.KeeramburKanagarajan@MyGeorgian.ca
    suryaprakash.vadivel@mygeorgian.ca

AIM: The goal of this project is to build and test a new computer vision model that estimates depth from a single image. The model uses a special structure with a smart way of looking at the whole picture and a quick method for figuring out the depth details. We want to make sure our version of the model works as well as the one described in the paper, especially on a tricky dataset called NYU Depth V2. We also aim to understand how the model works, make it run faster, and check if a specific technique for improving the model is effective. Once the code for the model is available, we plan to help make it easier for others to use and improve. Overall, we're trying to make this depth estimation model better and more useful for different situations.

Git repo Link: [Original Repo link](https://github.com/vinvino02/GLPDepth.git)

Paper Description : The paper introduces a novel architecture, Global-Local Path Networks with Vertical CutDepth, for monocular depth estimation using convolutional neural networks. It leverages a hierarchical transformer encoder to capture global context and a lightweight yet powerful decoder with selective feature fusion for local connectivity, achieving state-of-the-art performance on the NYU Depth V2 dataset. The proposed model shows improved generalization ability and robustness, outperforming other comparative models while maintaining lower computational complexity.

Problem Statement : The current implementation of the monocular depth estimation model, Global-Local Path Networks with Vertical CutDepth, may face limitations in providing detailed image outputs.

Context of the problem: The given code is not providing the clear visualisation in detail so we are going to contribute some detailed visualisation with some comparison and with colour bar. Because this is basically a visualisation model aims to to provide the depth estimation using a CNN powered architecture so it is essential to provide detailed visualisation to this model.

Limitations about other approaches: So for this paper we are gonna contribute some detailed visualisation which has not have any limitations in approach. Because the way people plans to visualise an image and its depth areas are their own ideas.

Solution : So the present code consist of only the depth areas with a different colour but we are going to add a comparison between original image and processed image and also we are going to add a colour bar that will be used to identify the range of depth

Methodology:
So we are gonna implement a colorbar for more understanding and also a comparison
"""



"""IMPLEMENTATION

![architecture](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/glpn_architecture.jpg)
"""

!pip install -q git+https://github.com/huggingface/transformers.git

!pip install -q opencv-python

from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation

feature_extractor = GLPNFeatureExtractor.from_pretrained("vinvino02/glpn-nyu")
model = GLPNForDepthEstimation.from_pretrained("vinvino02/glpn-nyu")

#preparing Image for the model
from PIL import Image
import requests

url = 'https://images.all-free-download.com/images/graphicwebp/simple_room_picture_167607.webp'
image = Image.open(requests.get(url, stream=True).raw)
image

pixel_values = feature_extractor(image, return_tensors="pt").pixel_values
print(pixel_values.shape)

import torch

with torch.no_grad():
  outputs = model(pixel_values)
  predicted_depth = outputs.predicted_depth

predicted_depth.shape

# interpolate to original size
prediction = torch.nn.functional.interpolate(
                    predicted_depth.unsqueeze(1),
                    size=pixel_values.shape[-2:],
                    mode="bicubic",
                    align_corners=False,
             )
prediction = prediction.squeeze().cpu().numpy()

import numpy as np
formatted = (prediction * 255 / np.max(prediction)).astype("uint8")
depth = Image.fromarray(formatted)
depth

import matplotlib.pyplot as plt

plt.imshow(prediction, cmap="jet")

import numpy as np
import cv2

def write_depth(depth, bits):
  depth_min = depth.min()
  depth_max = depth.max()

  max_val = (2 ** (8 * bits)) - 1

  if depth_max - depth_min > np.finfo("float").eps:
      out = max_val * (depth - depth_min) / (depth_max - depth_min)
  else:
      out = np.zeros(depth.shape, dtype=depth.dtype)

  cv2.imwrite("result.png", out.astype("uint16"), [cv2.IMWRITE_PNG_COMPRESSION, 0])

  return

write_depth(prediction, bits=2)

Image.open('result.png')

#We can also leverage OpenCV's COLORMAP_RAINBOW functionality for visualization:
pred_d_numpy = (prediction / prediction.max()) * 255
pred_d_numpy = pred_d_numpy.astype(np.uint8)
pred_d_color = cv2.applyColorMap(pred_d_numpy, cv2.COLORMAP_RAINBOW)
Image.fromarray(pred_d_color)

pred_d_numpy = (prediction / prediction.max()) * 255
pred_d_numpy = pred_d_numpy.astype(np.uint8)
pred_d_color = cv2.applyColorMap(pred_d_numpy, cv2.COLORMAP_RAINBOW)
Image.fromarray(pred_d_color)

plt.imshow(prediction, cmap="viridis")

# Add a colorbar for reference
plt.colorbar()

# Show the plot
plt.show()

import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))

ax1.imshow(image, cmap="viridis")
ax1.set_title('Original Image')
ax1.axis('off')


im = ax2.imshow(prediction, cmap="viridis")
ax2.set_title('Detailed View')
ax2.axis('off')
# Add a colorbar for reference
cbar = plt.colorbar(im, ax=ax2, orientation='vertical')

# Show the plot
plt.show()

"""# Conclusion and Future Direction

In conclusion, The architecture of the model is flawless for now so in future we can explore the architectures to keep up with the recent advancements

# References:

[1]:  Doyeon Kim, Woonghyun Ka, Pyunghwan Ahn, Donggyu Joo, Sewhan Chun, Junmo Kim,  Global-Local Path Networks for Monocular Depth Estimation
with Vertical CutDepth,2022
"""
