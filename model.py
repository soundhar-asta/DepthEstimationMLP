# -*- coding: utf-8 -*-
"""AIDI_1002_Final_Project_Template (3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ULo24Nb3lxR9zLHZgWWB4eNLvQjwWqQC

# Title: AIDI 1002 Final Term Project Report

#### Members' Names:
    Soundhar Keerambur Kanagarajan (200573263)
    Surya Prakash Vadivel (200573940)

####  Emails:
    Soundhar.KeeramburKanagarajan@MyGeorgian.ca
    suryaprakash.vadivel@mygeorgian.ca

AIM: The goal of this project is to build and test a new computer vision model that estimates depth from a single image. The model uses a special structure with a smart way of looking at the whole picture and a quick method for figuring out the depth details. We want to make sure our version of the model works well with the images with various sizes or resolution and also not prone to the network errors. We are aiming to provide more detailed visualisation to view the output or processed image in a more detailed manner.

Git repo Link: [Original Repo link](https://github.com/vinvino02/GLPDepth.git)

Paper Description : The paper introduces a novel architecture, Global-Local Path Networks with Vertical CutDepth, for monocular depth estimation using convolutional neural networks. It leverages a hierarchical transformer encoder to capture global context and a lightweight yet powerful decoder with selective feature fusion for local connectivity, achieving state-of-the-art performance on the NYU Depth V2 dataset. The proposed model shows improved generalization ability and robustness, outperforming other comparative models while maintaining lower computational complexity.

Problem Statement : The current implementation of the monocular depth estimation model, Global-Local Path Networks with Vertical CutDepth, may face network errors and its robustness might also be affected by irregular image size whilw training with large dataset.

Context of the problem: The current model might not be able to handle the network errors that are unhandled, and also some model are restricted with image's resolution or size which makes the model to crash or affects its robustness

Limitations about other approaches: So for this paper we are gonna contribute some detailed visualisation which has not have any limitations in approach. Because the way people plans to visualise an image and its depth areas are their own ideas.

Solution : The code uses try and except blocks to handle error during GLPN model and image loading. If an error occurs, it prints an error message specifying the issue, and a placeholder comment suggests possible actions for appropriate error handling, such as exiting the script or using default values. And also we have restricted the image size to (512,512) to make it more robust.we have also added some detailed visualisation and comparison between the original and the processed image.

Methodology:
So we are gonna implement the error handling by try, except and we are restricting the image size by using entering a custom resolution that we need.

IMPLEMENTATION

![architecture](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/glpn_architecture.jpg)
"""

!pip install -q git+https://github.com/huggingface/transformers.git

!pip install -q opencv-python

from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation

try:
    feature_extractor = GLPNFeatureExtractor.from_pretrained("vinvino02/glpn-nyu")
    model = GLPNForDepthEstimation.from_pretrained("vinvino02/glpn-nyu")
except Exception as e:
    print(f"Error loading the model: {e}")
    # Handle the error appropriately
try:
    image = Image.open(requests.get(url, stream=True).raw)
except Exception as e:
    print(f"Error loading the image: {e}")
    # Handle the error appropriately

#preparing Image for the model
from PIL import Image
import requests

url = 'https://images.all-free-download.com/images/graphicwebp/simple_room_picture_167607.webp'
target_size = (512, 512)  # Adjust according to the model's input size
image = image.resize(target_size, Image.ANTIALIAS)
print("image size",image.size)

pixel_values = feature_extractor(image, return_tensors="pt").pixel_values
print(pixel_values.shape)

import torch

with torch.no_grad():
  outputs = model(pixel_values)
  predicted_depth = outputs.predicted_depth

predicted_depth.shape

# interpolate to original size
prediction = torch.nn.functional.interpolate(
                    predicted_depth.unsqueeze(1),
                    size=pixel_values.shape[-2:],
                    mode="bicubic",
                    align_corners=False,
             )
prediction = prediction.squeeze().cpu().numpy()

import numpy as np
formatted = (prediction * 255 / np.max(prediction)).astype("uint8")
depth = Image.fromarray(formatted)
depth

import matplotlib.pyplot as plt

plt.imshow(prediction, cmap="jet")

import numpy as np
import cv2

def write_depth(depth, bits):
  depth_min = depth.min()
  depth_max = depth.max()

  max_val = (2 ** (8 * bits)) - 1

  if depth_max - depth_min > np.finfo("float").eps:
      out = max_val * (depth - depth_min) / (depth_max - depth_min)
  else:
      out = np.zeros(depth.shape, dtype=depth.dtype)

  cv2.imwrite("result.png", out.astype("uint16"), [cv2.IMWRITE_PNG_COMPRESSION, 0])

  return

write_depth(prediction, bits=2)

Image.open('result.png')

#We can also leverage OpenCV's COLORMAP_RAINBOW functionality for visualization:
pred_d_numpy = (prediction / prediction.max()) * 255
pred_d_numpy = pred_d_numpy.astype(np.uint8)
pred_d_color = cv2.applyColorMap(pred_d_numpy, cv2.COLORMAP_RAINBOW)
Image.fromarray(pred_d_color)

pred_d_numpy = (prediction / prediction.max()) * 255
pred_d_numpy = pred_d_numpy.astype(np.uint8)
pred_d_color = cv2.applyColorMap(pred_d_numpy, cv2.COLORMAP_RAINBOW)
Image.fromarray(pred_d_color)

plt.imshow(prediction, cmap="viridis")

# Add a colorbar for reference
plt.colorbar()

# Show the plot
plt.show()

import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))

ax1.imshow(image, cmap="viridis")
ax1.set_title('Original Image')
ax1.axis('off')


im = ax2.imshow(prediction, cmap="viridis")
ax2.set_title('Detailed View')
ax2.axis('off')
# Add a colorbar for reference
cbar = plt.colorbar(im, ax=ax2, orientation='vertical')

# Show the plot
plt.show()

"""# Conclusion and Future Direction

In conclusion, The architecture of the model is flawless for now so in future we can explore the architectures to keep up with the recent advancements

# References:

[1]:  Doyeon Kim, Woonghyun Ka, Pyunghwan Ahn, Donggyu Joo, Sewhan Chun, Junmo Kim,  Global-Local Path Networks for Monocular Depth Estimation
with Vertical CutDepth,2022
"""
